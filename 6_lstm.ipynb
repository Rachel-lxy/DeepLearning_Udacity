{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.292499 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.91\n",
      "================================================================================\n",
      "elkain onrs    q olsyifmupeentetgitdipa i ukgb wnea cqoctye kyrlfq xbm y ujlerde\n",
      "pdleqiireftkk  c vuusmuanr ftacd kviimrcyjoerwr ctnnsfawtv mbdrgdmi  ksfeegaw no\n",
      "zlpaeqta inbjnooqzw t  k depduzusm nsaf x  nt oyogzipoadn shcaxgtvsryngndcoi h  \n",
      "oedzkcvojyataee zi edove adp  c  iditabkmeiortqexngrgauci r hssc htjythvrqith za\n",
      "bghoiescofotnrpvdaa fcpal itovwbsrmlielmgepewi z dwgeins ilgiaf dyowddhkuqewfp p\n",
      "================================================================================\n",
      "Validation set perplexity: 20.17\n",
      "Average loss at step 100: 2.587472 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.04\n",
      "Validation set perplexity: 10.56\n",
      "Average loss at step 200: 2.249346 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.51\n",
      "Validation set perplexity: 8.64\n",
      "Average loss at step 300: 2.103277 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.59\n",
      "Validation set perplexity: 8.29\n",
      "Average loss at step 400: 2.005092 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.57\n",
      "Validation set perplexity: 8.03\n",
      "Average loss at step 500: 1.943696 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.83\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 600: 1.917402 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 700: 1.866052 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 800: 1.825739 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 900: 1.837030 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.07\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 1000: 1.833759 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "================================================================================\n",
      "zer treassion and suce is not eder atonters reateven ffatles was defers a derikh\n",
      "ch yecaully xanes frov the critter promq and evelate of bindennic from contracal\n",
      "ne in reuans ne the sell to bostory butgred in soluc deery whatyon lezge trounkl\n",
      "xing presuleh he rubal the indiele vistout wo seric the sitirs theor the favilid\n",
      "y beligioter fols lussyed the usied five kive mmalolin mimines ante in deach ver\n",
      "================================================================================\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 1100: 1.780864 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 1200: 1.760188 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1300: 1.735948 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1400: 1.749198 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1500: 1.741235 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1600: 1.746595 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1700: 1.716149 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 1800: 1.678546 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1900: 1.653218 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2000: 1.701759 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "================================================================================\n",
      "wers and was tyobel of specbarces number a ganking the rass the english beading \n",
      " intory fustory when peas at itring x see hould on s perking he pairenters priva\n",
      "rade or syrequal gram the maincries a franside englining chartermalilifs of char\n",
      "archies as rociniod in brundher that wands usaghtwogn deciod theal hindu antimit\n",
      "ews courophart amamper at the seaze frysion is as o regentiaghing rosodizy is we\n",
      "================================================================================\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2100: 1.689682 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 2200: 1.681862 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 2300: 1.643163 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2400: 1.662300 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2500: 1.684001 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 2600: 1.658215 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2700: 1.661176 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2800: 1.655683 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 2900: 1.656466 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3000: 1.653933 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      "he alsevele and geadto in ob is sachband cope criss s sorth apingle was his is s\n",
      "c initions of incordisue fow abyouc in the could by the movicy real foluther and\n",
      "zer acculses before wholies of slotlentes and unation ecotist it aira s insinted\n",
      "and clealinged the passuated is libchtral s ivered than not inslactic arcidulal \n",
      "istlith offessing was constriu the lagatascon nationally nork populages as and k\n",
      "================================================================================\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3100: 1.631353 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3200: 1.648866 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3300: 1.641583 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 3400: 1.666932 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3500: 1.660293 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3600: 1.671579 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3700: 1.647328 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3800: 1.647064 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3900: 1.638933 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4000: 1.658530 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "================================================================================\n",
      "ing person beth cented to previction of idels after mer the respion grophen was \n",
      "ger followed from was desonce of easion of the led batlhough majular oculdguach \n",
      "zopher sothonque some fromce tore medicianu with penal boynon thungizer with bla\n",
      "ous dowe light times lintle for fulding copmetional etophick have with it theser\n",
      "her parted hedote and dywich of the one yaschnes as rugard wame not afteore impe\n",
      "================================================================================\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4100: 1.637868 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4200: 1.640092 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4300: 1.616603 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4400: 1.611461 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.40\n",
      "Average loss at step 4500: 1.617818 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4600: 1.616528 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4700: 1.627822 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4800: 1.630544 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4900: 1.637274 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5000: 1.610240 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "================================================================================\n",
      "ferians purchwi of the itferthing manyita orgentary heme calmities is one nine s\n",
      "by region in them disproter the new five the ging has fiels basi affenents jehy \n",
      "wassamull boghwed foredou origned sinces to thear with one two zero zero zero fo\n",
      "larb sime singer may was freuty the sition the turkea divides in univil buss in \n",
      "cive two nine six supp c japaneble or falled to a caunta congesty excoppers writ\n",
      "================================================================================\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5100: 1.606376 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5200: 1.590091 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5300: 1.582513 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5400: 1.587013 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5500: 1.572525 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5600: 1.581092 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5700: 1.573546 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5800: 1.583977 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5900: 1.575828 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6000: 1.550512 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "j more of may maans a deceavarlows c under frym team age the also on alls state \n",
      "vape model for they construction point three six one nine three two empatic thre\n",
      "orgua holent was rostical sogelice slyma both estalilew for stronces was if ven \n",
      "ging the medaling forms profession on whania chirets crot strami been crass sing\n",
      "hic at the froctire to be uart an embort nation dohable st touranso most of the \n",
      "================================================================================\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6100: 1.567522 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6200: 1.538763 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6300: 1.548381 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6400: 1.544071 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6500: 1.559295 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6600: 1.602843 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6700: 1.588968 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6800: 1.609405 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6900: 1.591177 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 7000: 1.581033 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "ber one nine two five eight one one five viduourg the argiaca one nine six sentr\n",
      "zern on for become as sonjenger was lowarly yanisonciamenes hwop limites to the \n",
      "ra he comer of well author take many shipphal a ba by vess levert fherean in cen\n",
      "ing an axasism alter fiest work of hyly law was allow lass king caspe bechengera\n",
      "quering the entrus part in deving french for bolk from the consesutations then f\n",
      "================================================================================\n",
      "Validation set perplexity: 4.26\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Concatenate the parameters of input gate, forget gate, memory cell and output gate into a single matrix\n",
    "  px = tf.concat([ix, fx, cx, ox], 1) # vocabulary_size * (4 * num_nodes)\n",
    "  pm = tf.concat([im, fm, cm, om], 1) # num_nodes * (4 * num_nodes)\n",
    "  pb = tf.concat([ib, fb, cb, ob], 1) # 1 * (4 * num_nodes)\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    # Multiply input and previous output by the single matrix of parameters and add the biases\n",
    "    res = tf.matmul(i, px) + tf.matmul(o, pm) + pb # 1 * (4 * num_nodes)\n",
    "    input, forget, update, output = tf.split(res, [num_nodes] * 4, 1)\n",
    "    input_gate, forget_gate, output_gate = tf.sigmoid(input), tf.sigmoid(forget), tf.sigmoid(output)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296838 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.03\n",
      "================================================================================\n",
      "yrabeideeedtf qir  tpks   aqf x hiht rela  etxfe  arduqgcrtne bxoqketmh  rypxudc\n",
      "yiyhq petdehxnohhp becpanettmdarhaehxx eqjz  rpietuvwecsgy a nqi x iculswti jrpk\n",
      "bss ub rucjr efhssuc s sexv gi hesenmgmtvfrjxep omemwaduijnefnvtdea bphclxge oo \n",
      "av   hae nsepkdzxcotg qbn ijjqlldmpjm hjeasf h  amreriemhsaw tlchdirfnchrurfm ke\n",
      "etioipjpaeq aerr  xr aybkteidt pt einawjp mek n oejoadt gnwfor ewe zu gt lakbsee\n",
      "================================================================================\n",
      "Validation set perplexity: 19.97\n",
      "Average loss at step 100: 2.589740 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.84\n",
      "Validation set perplexity: 10.49\n",
      "Average loss at step 200: 2.249273 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.44\n",
      "Validation set perplexity: 8.87\n",
      "Average loss at step 300: 2.089527 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 400: 2.034656 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.77\n",
      "Validation set perplexity: 7.82\n",
      "Average loss at step 500: 1.978379 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 600: 1.894453 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 700: 1.868349 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.07\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 800: 1.864751 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 900: 1.841671 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 1000: 1.845075 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "================================================================================\n",
      "porstor d one nine two one one one five seva to refbengs is dulafd the nox becat\n",
      "isties d one seven one zeree p ic starefor of iplome aid nounal hyseam free umbi\n",
      "queted have kn coltury chpsingy qusider sipence ducciss ty lan p st agmments scl\n",
      "orir one beegahly abvitaphic sioneration himbersisus varyure athon tecriduring i\n",
      "x goundever posed cappullaged amortry an onf insixia pymon arcanfly in condic wi\n",
      "================================================================================\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 1100: 1.795874 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 1200: 1.768726 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 1300: 1.756543 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1400: 1.759163 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 1500: 1.743582 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 1600: 1.728175 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1700: 1.712987 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 1800: 1.690641 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1900: 1.691407 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2000: 1.678079 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "x taf sed includeds the a deswunce or rativia which outheven of desirina wicouse\n",
      "h included stile agrieticals artitle to concroduse life this reforence are hode \n",
      "bida form leke filt and govern as not in this secreeds cave in the bolies as des\n",
      "figed to known panion equece will and fight than one nine five nine nine five ze\n",
      "jand galects also difpedies and sup after the ise indest chesestian astrection a\n",
      "================================================================================\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2100: 1.685211 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2200: 1.708231 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2300: 1.706368 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2400: 1.688424 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2500: 1.693714 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2600: 1.672870 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 2700: 1.683521 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2800: 1.682242 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2900: 1.672867 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3000: 1.688359 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "================================================================================\n",
      "s georcn for sea our ofgnitmals if tenrize the freete as extrade crafpe see low \n",
      "f the trugh the yetwer to will guirfam time shongwions by hir the one nine six s\n",
      "quest neccurate to bosts ache winther souted the iartes la kear a in of the sepe\n",
      "h trans of tusious as and form it partys well lenk to one six nine nine zero zer\n",
      "ith encem or he a durrahy new other atouble suppleidian one two zero zero five o\n",
      "================================================================================\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3100: 1.650725 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3200: 1.632994 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 3300: 1.646895 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3400: 1.632146 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3500: 1.674402 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3600: 1.655608 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3700: 1.652605 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3800: 1.656734 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 3900: 1.651608 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4000: 1.644151 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "================================================================================\n",
      "lar other of systric regnature with koinger series the edical peover s stamspers\n",
      "weiply rewins in in howeve was autoxicia trde the swe was destine includest from\n",
      " thens from moderics is a forcer preside associated athain the creares for wheye\n",
      "cess bort of the leading is enarge of the dulew this be rewoum a whl capalizs th\n",
      "vers gackas assists courtames zerrevin tage of fren in transfering change offenc\n",
      "================================================================================\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4100: 1.620974 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4200: 1.614440 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4300: 1.613412 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4400: 1.605493 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4500: 1.639508 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4600: 1.623357 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 4700: 1.622326 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4800: 1.606040 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4900: 1.618223 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5000: 1.613570 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "================================================================================\n",
      "jate connectly degitl bunging x was reporticiation of rinchinest used that not c\n",
      "very charaht armounce world times chinal liberates peart liewer it affairles to \n",
      "p teeping zan obel everty of brugace in jew was for amels has and that actumbece\n",
      "les deew accept of the embiply west three nine three remains quuarg of meneum ap\n",
      "mets protect aslla in the senovs awcal rack in it one nine of four one faview ke\n",
      "================================================================================\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5100: 1.587947 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5200: 1.593357 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5300: 1.588653 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5400: 1.589470 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5500: 1.587725 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5600: 1.561716 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5700: 1.579559 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5800: 1.596774 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5900: 1.575835 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6000: 1.584037 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "================================================================================\n",
      " of one nine eight six eight psicationalls two fawled and the as bestert to mama\n",
      "mark grounds as dideovan comilitican state a toll reporter called two gildhress \n",
      "x willed a minisical nai smetlog a c were thas the editiquelized one one one two\n",
      "oud forming heavica bradmous puyces in made and h s irisher s groughts celtiam a\n",
      "e war member greek ampubation not in of as a beads also again atticly of were ce\n",
      "================================================================================\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6100: 1.571011 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6200: 1.584506 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6300: 1.580869 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6400: 1.566986 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6500: 1.552947 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6600: 1.592870 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6700: 1.568669 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6800: 1.574448 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6900: 1.571529 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 7000: 1.587962 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "x intername docorr pense wunce polining perimination one eight two senzii dectar\n",
      "f and agay populand new in the techsine hight of the simplory albancove boldward\n",
      "s which statish finations complex distine in is by may rain some sed well gling \n",
      "hagist in fine f a bithic stack of bittle it is methic thro t independing howeve\n",
      "k prole these napilar morive also unities about s the uch irish compresegn it ma\n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's do (a) - introduce an embedding lookup on the single character inputs and feed it to the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Character embeddings.\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i_embedding, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i_embedding, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i_embedding, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i_embedding, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i_embedding, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    i_embedding = tf.nn.embedding_lookup(embeddings, tf.arg_max(i, 1))\n",
    "    output, state = lstm_cell(i_embedding, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(embeddings, tf.arg_max(sample_input, 1))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.311964 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.44\n",
      "================================================================================\n",
      "wp ep if il kbdcerukia cmjztthdceiy l an qdwc nl tebdey p iacudwmbuohljkqr  zxxn\n",
      "zivia o do a  ecec ziwwbm slvqt a hhohhor tli sutai  qs es ofkatohg dlcrextfnhti\n",
      "xoqeir sx bs ma it i nsext n feg y exeslw  f ierilnqbuz  zjioa dhyiqa u emjltuo \n",
      "qmathjel    tn e treey on e  p g  waxerdn  wbt a  eq sesreset lqj g uovg edzrxtt\n",
      "lldsna s rzy  ignlt zhneees zttmjorr  t  hdbeejs zdmie  vvne rwajofdk vbhay drpq\n",
      "================================================================================\n",
      "Validation set perplexity: 19.96\n",
      "Average loss at step 100: 2.294640 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.12\n",
      "Validation set perplexity: 9.02\n",
      "Average loss at step 200: 2.019550 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 300: 1.919301 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 400: 1.867442 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 500: 1.888792 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 600: 1.822419 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 700: 1.805175 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 6.12\n",
      "Average loss at step 800: 1.793464 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 900: 1.782802 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 1000: 1.723837 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "================================================================================\n",
      "fic one nine eight six ninjed mostilibent fird tymnery were after ord one nine z\n",
      "aft in explack wiv seay vine beay was such is pladoroyight as they a posical wou\n",
      "ze cackation of they mutusiq apremically is it ssmelame produce it the sme exclu\n",
      "realta justate one one eight zero kecepth to act yetatrage beteruan to homoremy \n",
      "her resuschen pressence unif the more signsion new tai presimine inurporties f d\n",
      "================================================================================\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1100: 1.704778 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 1200: 1.732140 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1300: 1.715879 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1400: 1.691531 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1500: 1.687432 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 1600: 1.683668 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1700: 1.708032 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1800: 1.672291 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 1900: 1.679267 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2000: 1.691778 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "================================================================================\n",
      "bethn despate a related the populathod of nothemly the see congempangy activens \n",
      "ptershemble been states of the gacases one fiver boutan bapertions beeds sote ye\n",
      "x two enisters welbuser movies of kill jobray to a comblinated bittury sch boyal\n",
      "onatesist the resend for dratence to bready the karss jinuaces by transide the f\n",
      "heriquiated tory druksing baryaillitty roty comedus requates vaassoral overses e\n",
      "================================================================================\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2100: 1.680056 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2200: 1.656902 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2300: 1.666352 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2400: 1.666764 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2500: 1.692917 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2600: 1.665912 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 2700: 1.681105 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2800: 1.643670 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 2900: 1.650317 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3000: 1.656298 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "================================================================================\n",
      "ly a g alleus lamel duried sphoologial relleanse secled infembing illows if duri\n",
      "ines ard clains milonal convery with landon of a and necordy our litelly is flaw\n",
      "orion of sototy as four of otheral comemon island deseamy portivition s auchpool\n",
      "y unseer these orfacer arf virtit extern romer mofles is he scende the gersequel\n",
      "zed a posed south john misessor on aultad will formercer deceessim of was most p\n",
      "================================================================================\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3100: 1.655492 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3200: 1.650684 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3300: 1.637615 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 3400: 1.640010 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3500: 1.633271 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3600: 1.635848 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3700: 1.637307 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3800: 1.628315 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3900: 1.623636 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4000: 1.625864 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "================================================================================\n",
      "fers howev page pattures the brainshed to meads closesturetims algoring of a muc\n",
      "shopsypes is of wlarch ruding the recectors by sucted or voluder sounstraidaid m\n",
      "naintine ubersians at jealsore tuck of been ediate ptremaelthm is holday betwwat\n",
      "wks seventer a givenses ninezed of corm footer nine fout zero five two chessibul\n",
      "x a virt disbore has the scen max had for circlase of for allowhizatally or arit\n",
      "================================================================================\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 4100: 1.627250 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 4200: 1.615586 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 4300: 1.599983 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 4400: 1.633781 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4500: 1.638265 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 4600: 1.641560 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4700: 1.617033 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 4800: 1.601528 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 4900: 1.614449 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 5000: 1.639197 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "================================================================================\n",
      "maly since car broy kame in the at the may of finitionsaly the kingth bauth alic\n",
      "futance in the hrewser obsia a force to the jana associations cable his offer di\n",
      "jun cuptmia liness which plemessory mair the asoust in the ostion and mazth defe\n",
      "ly city one of mostinging would the whither anno conn the different of gundso an\n",
      "tine seard b grands jok one nine nine three by light an presidentia her north la\n",
      "================================================================================\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 5100: 1.628076 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 5200: 1.617055 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5300: 1.575144 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5400: 1.575043 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5500: 1.564821 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5600: 1.593892 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5700: 1.548898 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.93\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5800: 1.560040 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5900: 1.575669 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6000: 1.539890 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "================================================================================\n",
      "use about even abraticed a funnute wint was called b bructed in lakn trody n to \n",
      "gad them light to snobt cart one eight one nine seven two nottemet the churce th\n",
      "vation vial of the attemble imbconnal and as the many in the the apport anists s\n",
      "ge levela of atoneaoply agriousration haff electred were or incorent maky such h\n",
      "x belasure mexic quablenand bricted fluly a chowers of about a player operative \n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6100: 1.557272 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6200: 1.577870 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6300: 1.590620 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6400: 1.627707 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6500: 1.620205 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6600: 1.586016 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6700: 1.575880 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6800: 1.558874 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6900: 1.552370 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 7000: 1.561872 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "================================================================================\n",
      "klest socile and but nowyeght of but pervorence he with a tendefing k three diff\n",
      "ytraces one nine seven two zero five one from also tragpet and as those the elem\n",
      "x partious in two zero zero in as such organh of verting puket of the obsed one \n",
      "kets re to to draspically spered sustrience be dead bonk came from lides minorna\n",
      "fested to perwer one nine eight mem eight the scending in the left expersed pani\n",
      "================================================================================\n",
      "Validation set perplexity: 4.43\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's do (b) - convert the character LSTM above to a bigram-based LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Bigram embeddings.\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(bi_embedding, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(bi_embedding, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(bi_embedding, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(bi_embedding, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(bi_embedding, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = zip(train_data[:num_unrollings-1], train_data[1:num_unrollings])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for bi in train_inputs:\n",
    "    # Bigrams arranged in sequence like aa, ab, ac ... ba, bb, bc ...\n",
    "    bi_index = tf.arg_max(bi[0], 1) * vocabulary_size + tf.arg_max(bi[1], 1)\n",
    "    bi_embedding = tf.nn.embedding_lookup(embeddings, bi_index)\n",
    "    output, state = lstm_cell(bi_embedding, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = list()\n",
    "  for _ in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_input_index = tf.arg_max(sample_input[0], 1) * vocabulary_size + tf.arg_max(sample_input[1], 1)\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(embeddings, sample_input_index)\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.307919 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.33\n",
      "================================================================================\n",
      "gvjq njenr nit y pcve rcogyatgxatpr c b kz z igtsd b aredaeaeiejoa kp l ulasaxenx\n",
      "rhreztmtvulrj nzv vtxz tv ej fjdmwkdhe se csloavedfroc bwnlerwepaztajgiq ea  k az\n",
      "jnbtnpoyecnuiajocenoidihz s zy i vedyreihdqaeerzw eptwuqhraefu  nnkpruoow bqksha \n",
      "oelhuapes npetlanq tptwzgxdehtpto fcwrtl tba efr ig pfe se  lntfdfwehndtgeslncsav\n",
      "hwdpq cao aocsinenco kdenfofcmc einkjta ivxmsrre ywqut lacpiodmt  swnkrelxntrflvm\n",
      "================================================================================\n",
      "Validation set perplexity: 19.58\n",
      "Average loss at step 100: 2.269506 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.72\n",
      "Validation set perplexity: 8.62\n",
      "Average loss at step 200: 1.959273 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.75\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 300: 1.872653 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 400: 1.813505 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 500: 1.750997 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 600: 1.752187 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 700: 1.732336 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 800: 1.717319 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 900: 1.712296 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 1000: 1.683558 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "================================================================================\n",
      "zqauser which commonulyn as of shoor jechance one elessical limes knowns of the a\n",
      "fq schost of the in smell two km thas mean surveli adarles the someread aconw wil\n",
      "il moue cwns the un roves the fires reenoky somech from nicalution almated by and\n",
      "es appoinement of the ays other only paces often  egin ovex  exchangaltured axian\n",
      "bit out the alky in structior defhes begods otheral s new e stationnianic a compu\n",
      "================================================================================\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 1100: 1.689929 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 1200: 1.688096 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 1300: 1.686709 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 1400: 1.658209 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 1500: 1.649610 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 1600: 1.639276 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 1700: 1.645963 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 1800: 1.665245 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 1900: 1.649551 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 2000: 1.658855 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "================================================================================\n",
      "ie two five thousep town and maful gode advrs all on fissually or todas ermate in\n",
      "duca jam withris and stotp war the ice that implass atqa to elf stablish on malta\n",
      "ihare this a c perior stanction only rside sees on the in his as apartartionameli\n",
      "shist learruvil is the novemlos and charangel allector and a strail also the jlnu\n",
      " kemomerion and was and weald entitbach thsef might war one maintif fankel no nom\n",
      "================================================================================\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 2100: 1.646269 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 2200: 1.662887 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 2300: 1.644520 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 2400: 1.646773 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 2500: 1.649956 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 2600: 1.639729 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 2700: 1.622711 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 2800: 1.624778 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 2900: 1.621773 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 3000: 1.638312 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "================================================================================\n",
      "file instructions which in world steeph as smallegistions religiding will the mon\n",
      " that some coln used for cloving city the sulfc so reso frors tras envictical the\n",
      "ific rage wear was of reforencerd house to angeles of dature pynamcemologics of p\n",
      "tz and and theopologieds peopnrged a biographoss and these juch eary pala solgion\n",
      "phosve in baroque briters linife televil of drite and have one under with consotc\n",
      "================================================================================\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 3100: 1.613566 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 3200: 1.622780 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 3300: 1.624743 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 3400: 1.620627 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 3500: 1.604452 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 3600: 1.627715 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 3700: 1.592770 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 3800: 1.596811 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 3900: 1.584476 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 4000: 1.602930 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "================================================================================\n",
      "ky manion and one sevensevage one newefelgiestone have gast peasor galy raftodaye\n",
      "xponwind defined by douglassive the mountrelauw one nine fur five six six to the \n",
      "fnouts of matetiathlass testakent romans by that loked to the brcoise and eventio\n",
      "zdbully an album one four one nine six three of thigheated intradio stories of al\n",
      "sra procedaetereasion like of the partnermanyead distrengo and not eaple for thas\n",
      "================================================================================\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 4100: 1.618800 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 4200: 1.600545 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 4300: 1.566249 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 4400: 1.596884 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 6.76\n",
      "Average loss at step 4500: 1.576629 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 4600: 1.583651 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 4700: 1.600046 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 4800: 1.593019 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 4900: 1.612576 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 5000: 1.624069 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "================================================================================\n",
      "zwonly the oldere offt bruning that is the first have will forms herall and own e\n",
      "ka number but the arched ohins of the sice bection to a power the twitest profess\n",
      "ujen ums moong s shippylores by was after than resel other playedao or six konals\n",
      "dtar passom voreference government extre five tcep s and three to a law theired m\n",
      "udges striphereline from cation the replayer innetimes in pment hisn argument fro\n",
      "================================================================================\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 5100: 1.586021 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 5200: 1.593908 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 5300: 1.568111 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 5400: 1.559588 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 5500: 1.560732 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 5600: 1.541880 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 5700: 1.577039 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 5800: 1.566252 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 5900: 1.574620 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 6000: 1.538395 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "================================================================================\n",
      " years will from are eight nine three house propertain ii furtfy fol of a whiling\n",
      "j the bin a can be nine nine nine seven five eight one five nordigents best the o\n",
      "swinki coleant the last of idea herese tinder emitler for act to the war they acc\n",
      "    is auguries of center that they at ofswand see of which centner that about th\n",
      "py hambickise and peacary made one nine five textinney shortraning traditional is\n",
      "================================================================================\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 6100: 1.584881 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 6200: 1.583786 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 6300: 1.568661 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 6400: 1.580087 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 6500: 1.576537 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 6600: 1.569412 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 6700: 1.564953 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 6800: 1.570248 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 6900: 1.601747 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 7000: 1.586557 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "hmuccess doment othis counture petcyther codes movely direasm untp of the hemican\n",
      "zniely usually scrublish about player all all it born made a fit who on the fast \n",
      "xyth scene and atrale ory such left the new to descwhole one nine five ridge of l\n",
      "lve of company union facent toyf in these or become the he undernism the far webs\n",
      "lhlorism in the gael eventral vichs explishe people reements or when and official\n",
      "================================================================================\n",
      "Validation set perplexity: 6.35\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):      \n",
    "          # feed = sample(random_distribution())\n",
    "          # Generate initial bigram from random distribution\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):\n",
    "            feed.append(sample(random_distribution()))\n",
    "          sentence = ''\n",
    "          for i in range(2):\n",
    "            sentence += characters(feed[i])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input[0]: feed[0], sample_input[1]: feed[1]})\n",
    "            feed.append(sample(prediction))\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size): \n",
    "        b = valid_batches.next() # Now num_unrollings = 2 for bigram\n",
    "        predictions = sample_prediction.eval({sample_input[0]: b[0], sample_input[1]: b[1]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's do (c) - introduce dropout to LSTM. Based on the article, we should only apply dropout to the non-recurrent connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "  # Parameters:\n",
    "  # Bigram embeddings.\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))   \n",
    "  # Memory cell: input, state and bias.\n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))  \n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(bi_embedding, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(bi_embedding, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(bi_embedding, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(bi_embedding, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(bi_embedding, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "  train_inputs = zip(train_data[:num_unrollings-1], train_data[1:num_unrollings])   \n",
    "  train_labels = train_data[2:]\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for bi in train_inputs:\n",
    "    # Bigrams arranged in sequence like aa, ab, ac ... ba, bb, bc ...\n",
    "    bi_index = tf.arg_max(bi[0], 1) * vocabulary_size + tf.arg_max(bi[1], 1)\n",
    "    bi_embedding = tf.nn.embedding_lookup(embeddings, bi_index)\n",
    "    # Introduce dropout to input.\n",
    "    bi_embedding_dropout = tf.nn.dropout(bi_embedding, 0.9)\n",
    "    output, state = lstm_cell(bi_embedding_dropout, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    # Introduce dropout to output.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    logits_dropout = tf.nn.dropout(logits, 0.9)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels = tf.concat(train_labels, 0), logits=logits_dropout))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer =   optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = list()\n",
    "  for _ in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_input_index = tf.arg_max(sample_input[0], 1) * vocabulary_size + tf.arg_max(sample_input[1], 1)\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(embeddings, sample_input_index)\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.346925 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.75\n",
      "================================================================================\n",
      "lcc  tbxqvnkstho x eoeyspcwhlvo o mqv aeg nterc dps dm etn tjecfen pn lienjpd qs \n",
      "fbc vosy y ys qrbjhtq et qiauamrsglojmutjylamorf rrs  optqdaskeugg voqle bjttkjmt\n",
      "qeg i dx  cxoawimwaipsas ery gclenlj eog fcqwte efnx lsfglrefo jmkdoxg  yrewvriho\n",
      "mpnwejpl ndx vqt sgp tdqtq jent zirvm th amwm n yhr chqgenyj nnrpgt jg uno ern bl\n",
      "uqntqiasy  azaxekdfl h rpb pslnrvot ujzfxjoenuttoerajrqzewxveqdjaona su vh tnbndx\n",
      "================================================================================\n",
      "Validation set perplexity: 19.72\n",
      "Average loss at step 100: 2.434453 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.26\n",
      "Validation set perplexity: 9.35\n",
      "Average loss at step 200: 2.150856 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.30\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 300: 2.061359 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 7.89\n",
      "Average loss at step 400: 2.017937 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 500: 1.999131 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 7.83\n",
      "Average loss at step 600: 1.949581 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 700: 1.946481 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 800: 1.913379 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 900: 1.910481 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 1000: 1.899287 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "================================================================================\n",
      "ztrone only f bith mmention of two one nine nine zero proppist the crand christit\n",
      "ht favob spoxees act clanaties edics in the exturn eld starn joian relepces that \n",
      "yylphilize his the first one nine seven fwo one nazig four that ess isly chesssdu\n",
      "azsuching the ssra egrabu first iaocugth accest lath cost the scapepticidsh crepo\n",
      "szcdned claearnely disneaden it in two three nine seven one two two ynson ch babi\n",
      "================================================================================\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 1100: 1.890221 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 1200: 1.882270 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 1300: 1.865436 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 1400: 1.874001 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 1500: 1.895921 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 1600: 1.882025 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 1700: 1.862308 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 1800: 1.888890 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 1900: 1.889091 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 2000: 1.848372 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "================================================================================\n",
      "by of a detes a s which by musiflem aimmlezss its baamerican the linconingr radua\n",
      "lcusetter leat ballow intens for on draear which music the succonsrimas of u stud\n",
      "flinks radtter belsonvented from an a computer in krits veural irilenthmanrpositi\n",
      "sfleas also that to companyetion prirectrince sing of the times unition of the st\n",
      "zyland is is it vollycludeticle funline in formed implight the first fible are s \n",
      "================================================================================\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 2100: 1.848273 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 2200: 1.834974 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 2300: 1.869295 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 2400: 1.853034 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 2500: 1.840404 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 2600: 1.821141 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 2700: 1.816000 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 2800: 1.827190 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 2900: 1.810698 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 3000: 1.813987 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "================================================================================\n",
      "kjsuchologgs clet mildinger efvtkiment anot orwarilization bahand a madelum two c\n",
      "rs supplical out stok lative it as certter whu culs one laurei ooccess hotal roja\n",
      "nding his keywlibive remon the by cority by day from due that way ristate of supp\n",
      "zxish a thism differe latin tass the work lean is women medocitial betwn yetile d\n",
      " july kingary cliquose but in two three zero three two jwn storboan however fince\n",
      "================================================================================\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 3100: 1.837477 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 3200: 1.838377 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 3300: 1.825450 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 3400: 1.824367 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 3500: 1.813971 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 3600: 1.790727 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 3700: 1.798080 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 3800: 1.812646 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 3900: 1.826639 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 4000: 1.811884 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "================================================================================\n",
      "vy a busine rered to new a in themate christincipuelf four again genortkled inter\n",
      "has psh yeak chirmementive repreek breutorists ame perportreswellieveed preveloti\n",
      "rhinespecesses at coon demate and sfacesslitic hone are years for eight four nine\n",
      "wbes linettern http litorie rosexamplemented opes ninus brointle peers systems as\n",
      "ally toping its work not in diversiginal prose disades so in bolect espects winel\n",
      "================================================================================\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 4100: 1.820892 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 4200: 1.800499 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 4300: 1.801661 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 4400: 1.814459 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 6.76\n",
      "Average loss at step 4500: 1.808220 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 4600: 1.805830 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 4700: 1.798831 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 4800: 1.811316 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 4900: 1.804943 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 5000: 1.815107 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "================================================================================\n",
      "hnic in one zero five alhrradtuit that film system apollo one nine two two zero z\n",
      "zta fing x is forced suppeanion by at with scoti whether of expres defined the ce\n",
      "ning the one nine and centh your aclead the fatter views comparion or have start \n",
      "nvired however pictries ang hon from discotds montion language these the tempers \n",
      "ww are paders a wester constigratirement two convicq image rrong or nown similaxi\n",
      "================================================================================\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 5100: 1.795946 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 5200: 1.794513 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 5300: 1.790452 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 5400: 1.780366 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 5500: 1.774213 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 5600: 1.795148 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 5700: 1.781677 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 5800: 1.765326 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 5900: 1.778977 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 6000: 1.785466 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.96\n",
      "================================================================================\n",
      "on formal film child writish in and prefeeirade lion and in maint tradia as imccu\n",
      "vt a banerses availar shood on for what was been the greath the contraxically pho\n",
      "qfe for uphip up from for his live incente the pso was linus skythe major heavugh\n",
      "mhelm stodolifier for hourst as a was red mukor france ets of condaler gracults o\n",
      "ywrith not j gaint not adunsive sport on members after other lnovep land notp wil\n",
      "================================================================================\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 6100: 1.807471 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 6200: 1.776036 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 6300: 1.780707 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 6400: 1.807362 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 6500: 1.819923 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 6600: 1.794530 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 6700: 1.794609 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 6800: 1.774651 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 6900: 1.746575 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 7000: 1.790616 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.74\n",
      "================================================================================\n",
      "scast with the harmineast dvter domarcial with for wire of one nine five four pol\n",
      "exedviortal bunde vach of edular be old prorts a considiation exambruago we betwe\n",
      "ubs where the two nine three zero mittentire atberting statenned the karupn for d\n",
      "even of lemong infrii edly east by number most years to illitarying many cadition\n",
      "nnevel o behavudies action of the mounts yup terviewau darin one nine zero six on\n",
      "================================================================================\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 7100: 1.785353 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 7200: 1.782038 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 7300: 1.789142 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 7400: 1.782302 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 7500: 1.777214 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 7600: 1.769100 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 7700: 1.786992 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 7800: 1.798862 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 7900: 1.798081 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 8000: 1.790857 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "================================================================================\n",
      "ough clasde olgional bigerother one nine zero zero solied and dethe reposition ex\n",
      "kdf school these footh the degrandren phent ng has unwular through and and people\n",
      "  run inres party spincindent i inking phones geded jincress presidees to seury r\n",
      "xvater theospecuhlumenitary losited the riging clatones considered finands one fi\n",
      "vfades intrals squaly scient draw one nine acrains direscription populations solt\n",
      "================================================================================\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 8100: 1.762662 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 8200: 1.773894 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 8300: 1.783721 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 8400: 1.781138 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 8500: 1.791452 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 8600: 1.796826 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 8700: 1.789424 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 8800: 1.790357 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 8900: 1.770243 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 9000: 1.781173 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.61\n",
      "================================================================================\n",
      "nrope asle longs immediod cipher nic bood flongerature al close bankrupted serica\n",
      "gu exconceptke jain law larred dutericto character hison following would insends \n",
      "gkuwaut ensity deads of lithry that india number is one nine fanv invorld weeks f\n",
      "ia mixed mothages when qa side only assoning odpacer zero mia never the pulty til\n",
      "nxcarrity one nine in rehey also struggled a mestance two nine nine three in meld\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 6.41\n",
      "Average loss at step 9100: 1.783557 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 9200: 1.807973 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 9300: 1.793278 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 9400: 1.783607 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 9500: 1.788676 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 9600: 1.788630 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 9700: 1.793770 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 9800: 1.788289 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 9900: 1.769548 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 10000: 1.789274 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.94\n",
      "================================================================================\n",
      "zatejoses of citice s jewas the retuber informaties cases and very band gene for \n",
      "hambum of one nine nine s one zero s wildey didention infirst to affenrreader of \n",
      "qwar lief sklected in one seven by aimstorious nine deatl right lituring web econ\n",
      "mwars forma and s certates away faor rert leader province fiwrist to  saim by the\n",
      "ek in dicams of godubstil differs a personcer lessimilipidea many is supporld by \n",
      "================================================================================\n",
      "Validation set perplexity: 6.42\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):      \n",
    "          # feed = sample(random_distribution())\n",
    "          # Generate initial bigram from random distribution\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):\n",
    "            feed.append(sample(random_distribution()))\n",
    "          sentence = ''\n",
    "          for i in range(2):\n",
    "            sentence += characters(feed[i])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input[0]: feed[0], sample_input[1]: feed[1]})\n",
    "            feed.append(sample(prediction))\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size): \n",
    "        b = valid_batches.next() # Now num_unrollings = 2 for bigram\n",
    "        predictions = sample_prediction.eval({sample_input[0]: b[0], sample_input[1]: b[1]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "---------\n",
    "\n",
    "[1] https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/udacity\n",
    "\n",
    "[2] http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/\n",
    "\n",
    "[3] https://github.com/rndbrtrnd/udacity-deep-learning"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
