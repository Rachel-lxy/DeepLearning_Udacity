{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's add L2 regularization for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta = tf.placeholder(tf.float32)    \n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + beta * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here beta is a hyperparameter that needs to be tuned. I will try several beta values, and pick the optimal one which gives the highest accuracy on validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "beta_lst = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "valid_accuracy = []\n",
    "\n",
    "for b in beta_lst:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : b}\n",
    "            _, l, predictions = session.run(\n",
    "                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        valid_accuracy.append(accuracy(valid_prediction.eval(), valid_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I plot the accuracy on validation dataset corresponding to different beta values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEOCAYAAACaQSCZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSCBKGCIiEogShoiIBPL/seJoLR8WqOAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d915978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogx(beta_lst, valid_accuracy)\n",
    "plt.xlabel('beta')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0015848931924611173"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_lst[np.argmax(valid_accuracy)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beta = 0.0016 gives the highest accuracy on validation dataset. I will use it to train logistic regression again, and test it on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 23.873350\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 12.5%\n",
      "Minibatch loss at step 500: 2.671617\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 1000: 1.520974\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 1500: 0.814124\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 2000: 0.657473\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 2500: 0.750960\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 3000: 0.717209\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 81.9%\n",
      "Test accuracy: 89.1%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : 0.0016}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L2 regularization improves test accuracy of logistic regression from 86.4% in Assignment 2 to 89.1%. Now let's add L2 regularization for 1-hidden layer neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train this 1-hidden layer neural network with SGD\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset) \n",
    "  beta = tf.placeholder(tf.float32)    \n",
    "  \n",
    "  # Variables.\n",
    "  hidden_weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  hidden_biases = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  output_weights = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  output_biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  hidden_nodes = tf.nn.relu(tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases)\n",
    "  logits = tf.matmul(hidden_nodes, output_weights) + output_biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "    beta * (tf.nn.l2_loss(hidden_weights) + tf.nn.l2_loss(output_weights))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_hidden_nodes = tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(valid_hidden_nodes, output_weights) + output_biases)\n",
    "  test_hidden_nodes = tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(test_hidden_nodes, output_weights) + output_biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then tune beta as I did for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_accuracy = []\n",
    "\n",
    "for b in beta_lst:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : b}\n",
    "            _, l, predictions = session.run(\n",
    "                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        valid_accuracy.append(accuracy(valid_prediction.eval(), valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEOCAYAAACEiBAqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZh9x8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x129b78e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(beta_lst, valid_accuracy)\n",
    "plt.xlabel('beta')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0012589254117941701"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_lst[np.argmax(valid_accuracy)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, beta = 0.0013 gives the highest accuracy on validation dataset. I will use it to train 1-hidden layer neural network again, and test it on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 737.835083\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 20.1%\n",
      "Minibatch loss at step 500: 221.370239\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 1000: 110.800758\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 1500: 56.822105\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 2000: 29.571346\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 2500: 15.660949\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 3000: 8.503495\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.7%\n",
      "Test accuracy: 93.2%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : 0.0013}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L2 regularization improves test accuracy of 1-hidden layer neural network from 89.5% in Assignment 2 to 93.2%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will do this experiment on neural network without regularization, only using 2 batches to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train this 1-hidden layer neural network with SGD\n",
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  hidden_weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  hidden_biases = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  output_weights = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  output_biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  hidden_nodes = tf.nn.relu(tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases)\n",
    "  logits = tf.matmul(hidden_nodes, output_weights) + output_biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_hidden_nodes = tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(valid_hidden_nodes, output_weights) + output_biases)\n",
    "  test_hidden_nodes = tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(test_hidden_nodes, output_weights) + output_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 405.398438\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 31.0%\n",
      "Minibatch loss at step 500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.7%\n",
      "Minibatch loss at step 1000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.7%\n",
      "Minibatch loss at step 1500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.7%\n",
      "Minibatch loss at step 2000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.7%\n",
      "Minibatch loss at step 2500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.7%\n",
      "Minibatch loss at step 3000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.7%\n",
      "Test accuracy: 80.0%\n"
     ]
    }
   ],
   "source": [
    "num_batches = 2\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    # Only use two batches here to train the model.\n",
    "    offset = ((step % num_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loss reaches 0 at an early stage, however, as the training data is too small, overfitting occurs and the model fails to generalize to new data, as evidenced by low test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train this 1-hidden layer neural network with SGD\n",
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  hidden_weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  hidden_biases = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  output_weights = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  output_biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # Introduce dropout on the hidden layer.\n",
    "  hidden_nodes = tf.nn.dropout((tf.nn.relu(tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases)), 0.5)\n",
    "  logits = tf.matmul(hidden_nodes, output_weights) + output_biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_hidden_nodes = tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(valid_hidden_nodes, output_weights) + output_biases)\n",
    "  test_hidden_nodes = tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(test_hidden_nodes, output_weights) + output_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 449.548218\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 18.8%\n",
      "Minibatch loss at step 500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.3%\n",
      "Minibatch loss at step 1000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 1500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 2000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 2500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 3000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.9%\n",
      "Test accuracy: 82.9%\n"
     ]
    }
   ],
   "source": [
    "num_batches = 2\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = ((step % num_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network becomes more robust and performs better at generalizing to new data, as evidenced by higher test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will use two ways at the same time to achieve a better performance:\n",
    "\n",
    "- Add two more hidden layers in addition to the first hidden layer\n",
    "- Use learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train this 1-hidden layer neural network with SGD\n",
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "  \n",
    "  # Variables.\n",
    "  hidden_weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes1], stddev = 0.02))\n",
    "  hidden_biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  hidden_weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev = 0.02))\n",
    "  hidden_biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  hidden_weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev = 0.02))\n",
    "  hidden_biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  output_weights = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev = 0.02))\n",
    "  output_biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  hidden_nodes1 = tf.nn.relu(tf.matmul(tf_train_dataset, hidden_weights1) + hidden_biases1)\n",
    "  hidden_nodes2 = tf.nn.relu(tf.matmul(hidden_nodes1, hidden_weights2) + hidden_biases2)\n",
    "  hidden_nodes3 = tf.nn.relu(tf.matmul(hidden_nodes2, hidden_weights3) + hidden_biases3)\n",
    "  logits = tf.matmul(hidden_nodes3, output_weights) + output_biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 3000, 0.8, staircase = True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_hidden_nodes1 = tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights1) + hidden_biases1)\n",
    "  valid_hidden_nodes2 = tf.nn.relu(tf.matmul(valid_hidden_nodes1, hidden_weights2) + hidden_biases2)\n",
    "  valid_hidden_nodes3 = tf.nn.relu(tf.matmul(valid_hidden_nodes2, hidden_weights3) + hidden_biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(valid_hidden_nodes3, output_weights) + output_biases)\n",
    "  test_hidden_nodes1 = tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights1) + hidden_biases1)\n",
    "  test_hidden_nodes2 = tf.nn.relu(tf.matmul(test_hidden_nodes1, hidden_weights2) + hidden_biases2)\n",
    "  test_hidden_nodes3 = tf.nn.relu(tf.matmul(test_hidden_nodes2, hidden_weights3) + hidden_biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(test_hidden_nodes3, output_weights) + output_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.302873\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 9.5%\n",
      "Minibatch loss at step 1000: 0.469952\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 2000: 0.272188\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 3000: 0.355145\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 4000: 0.253991\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 5000: 0.304947\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 6000: 0.371075\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 7000: 0.357258\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 8000: 0.357800\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 9000: 0.223471\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 10000: 0.174483\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 11000: 0.153352\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 12000: 0.232848\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 13000: 0.208542\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 14000: 0.187582\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 15000: 0.120089\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.8%\n",
      "Test accuracy: 96.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 15001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 1000 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "---------\n",
    "\n",
    "[1] https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/udacity\n",
    "\n",
    "[2] https://www.oreilly.com/learning/not-another-mnist-tutorial-with-tensorflow\n",
    "\n",
    "[3] https://github.com/rndbrtrnd/udacity-deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test accuracy reaches 96.0%!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
